{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Google Colab 에서 NVIDIM NIM 테스트"
      ],
      "metadata": {
        "id": "a4LMAoWMZtJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. OpenAI 컴포넌트 설치"
      ],
      "metadata": {
        "id": "iSYqS2QoZ891"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYtmeWfwFplU"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. OpenAI 사용 선언 및 Google Colab의 Secret Key 사용을 위한 userdata 선언"
      ],
      "metadata": {
        "id": "Jat0OVnbaH_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "fiaRFLklHAtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. NIM의 base_url 주소와 Google Colab의 Secret Key로 저장한 NVIDIA_NIM 변수를 api_key 변수로 저장, 이때 NVIDIA_NIM의 Value 값은 Generate API key 설정.    "
      ],
      "metadata": {
        "id": "vDzUsa7SaFPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
        "  api_key = userdata.get('NVIDIA_NIM')\n",
        ")\n"
      ],
      "metadata": {
        "id": "R824wV9JHFNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. OpenAI API를 사용하여 “meta/llama3-8b-instruct”라는 모델을 호출하고 사용자 질문에 대한 스트리밍 응답을 생성하기\n",
        "\n",
        "##### model : OpenAI가 제공한 8B(80억) 파라미터 크기의 Llama3 기반 모델\n",
        "##### message : 대화 메시지\n",
        "##### 1) role : 메시지의 역할로 \"user\"로 설정\n",
        "##### 2) content : 사용자의 질문(내용) 전달\n",
        "##### 3) temperature : 출력의 창의성. 낮은 값(0~0.5): 균형적인 응답(덜 창의적)\n",
        "##### 4) top_p : Nucleus Sampling 기법으로 출력 확률 분포를 제어. 0.7로 설정하면 상위 80% 확률에 속하는 토큰만 고려\n",
        "##### 5) max_tokens : 응답에 사용할 최대 토큰 수. 여기서는 1024로 설정되어, 최대 1024개의 토큰을 생성\n",
        "##### 6) stream : 스트리밍 방식 여부 설정. True: 점진적으로 응답 받음. False: 모든 응답을 한 번에 받음.\n",
        "#####"
      ],
      "metadata": {
        "id": "P-FJtaU5eGhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fg7QXSgOfbB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"meta/llama-3.1-8b-instruct\",\n",
        "  messages=[{\"role\":\"user\",\"content\":\"who is Jensen Huang?\"}],\n",
        "  temperature=0.5,\n",
        "  top_p=0.8,\n",
        "  max_tokens=1024,\n",
        "  stream=True\n",
        ")\n",
        "\n",
        "# API 응답이 여러 “청크”로 나뉘어 들어오므로, 각 청크를 순회하며 처리함.\n",
        "for chunk in completion:\n",
        "  if chunk.choices[0].delta.content is not None: # API 응답에서 현재 청크의 텍스트 내용 추출\n",
        "    print(chunk.choices[0].delta.content, end=\"\") # 출력된 텍스트를 화면에 표시하며, 줄바꿈 없이 이어서 출력(end=\"\").\n"
      ],
      "metadata": {
        "id": "KvJZIV7aHMHK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}